# Deep-learning-for-sequential-data-GPT-and-ViT

** Problem 2 GPT


This notebook  contains:

* Implementation of the GPT model.
* Training the GPT model on a given dataset.
* Generating sequences using the trained model.
* Evaluation of the generated sequences.


** Problem 3 Vision Transformer

* Vision Transformer
* Simple CNN
* Shifted Patch Tokenization (SPT)
* Locality Self-Attention (LSA)

## Requirements

To run the notebook, you will need the following libraries:

* `torch`
* `transformers` (huggingface)
* `datasets` (huggingface)
* `accelerate` (huggingface)
* other common libraries like `numpy`, `matplotlib`, etc. (install as needed)

You can install the necessary packages using pip:

```bash
pip install torch transformers datasets accelerate
